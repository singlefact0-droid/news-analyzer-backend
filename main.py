from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from huggingface_hub import InferenceClient

# -----------------------------
# Directly include your Hugging Face token here
HF_TOKEN = "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"
# -----------------------------

# Create Hugging Face client
client = InferenceClient(token=HF_TOKEN)

# Create FastAPI app
app = FastAPI()

# Enable CORS for your frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://house-of-prompts.web.app"],  # frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request model
class Article(BaseModel):
    article: str

# Analyze route
@app.post("/analyze")
async def analyze(article: Article):
    try:
        text = article.article
        if not text.strip():
            return {"credibility_score": None, "summary": "Empty input.", "counterarguments": "N/A"}

        prompt = f"""
        Analyze the credibility of this news article and give:
        1. Credibility score out of 100%
        2. Short summary
        3. Counterarguments against its claims

        Article: {text}
        """

        # Correct usage of Hugging Face InferenceClient
        response = client.text_generation(
            model="gpt2",  # Replace with a better HF model later if desired
            prompt=prompt,
            max_new_tokens=200
        )

        output = response.generated_text if hasattr(response, "generated_text") else response[0]['generated_text']

        return {
            "credibility_score": "AI-generated",
            "summary": output,
            "counterarguments": "Generated by AI"
        }

    except Exception as e:
        # Return error details instead of crashing
        return {"error": str(e)}

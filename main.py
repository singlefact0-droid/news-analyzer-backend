from fastapi import FastAPI
from pydantic import BaseModel
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

# Load HF token from .env
load_dotenv()
HF_TOKEN = os.getenv("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")

# Create Hugging Face client
client = InferenceClient(token=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)

# 1️⃣ Define FastAPI app
app = FastAPI()

# 2️⃣ Define request model
class Article(BaseModel):
    article: str

# 3️⃣ Define route
@app.post("/analyze")
async def analyze(article: Article):
    text = article.article
    if not text.strip():
        return {"credibility_score": None, "summary": "Empty input.", "counterarguments": "N/A"}

    prompt = f"""
    Analyze the credibility of this news article and give:
    1. Credibility score out of 100%
    2. Short summary
    3. Counterarguments against its claims

    Article: {text}
    """

    response = client.text_generation(
        model="gpt2",  # Replace with any suitable HF model
        inputs=prompt,
        max_new_tokens=200
    )

    output = response.generated_text if hasattr(response, "generated_text") else response[0]['generated_text']

    return {
        "credibility_score": "AI-generated",
        "summary": output,
        "counterarguments": "Generated by AI"
    }

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from huggingface_hub import InferenceClient

# -----------------------------
# Directly include your Hugging Face token here
HF_TOKEN = "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"
# -----------------------------

# Create Hugging Face client
client = InferenceClient(token=HF_TOKEN)

# Create FastAPI app
app = FastAPI()

# Enable CORS for your frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://house-of-prompts.web.app"],  # frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request model
class Article(BaseModel):
    article: str

# Analyze route
@app.post("/analyze")
async def analyze(article: Article):
    try:
        text = article.article
        if not text.strip():
            return {"credibility_score": None, "summary": "Empty input.", "counterarguments": "N/A"}

        prompt = f"""
        Analyze the credibility of this news article and give:
        1. Credibility score out of 100%
        2. Short summary
        3. Counterarguments against its claims

        Article: {text}
        """

        # Hugging Face text generation
        response = client.text_generation(
            model="gpt2",
            prompt=prompt,
            max_new_tokens=200
        )

        # Safely extract generated text
        if isinstance(response, list) and 'generated_text' in response[0]:
            output = response[0]['generated_text']
        elif hasattr(response, "generated_text"):
            output = response.generated_text
        else:
            output = "Could not generate text."

        return {
            "credibility_score": "AI-generated",
            "summary": output,
            "counterarguments": "Generated by AI"
        }

    except Exception as e:
        return {"error": str(e)}


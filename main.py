from huggingface_hub import InferenceClient

# Use your Hugging Face token here
HF_TOKEN = "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"
client = InferenceClient(token=HF_TOKEN)

@app.post("/analyze")
async def analyze(article: Article):
    text = article.article
    if not text.strip():
        return {"credibility_score": None, "summary": "Empty input.", "counterarguments": "N/A"}

    prompt = f"""
    Analyze the credibility of this news article and give:
    1. Credibility score out of 100%
    2. Short summary
    3. Counterarguments against its claims

    Article: {text}
    """

    response = client.text_generation(
        model="gpt2",  # Replace with any suitable HF model
        inputs=prompt,
        max_new_tokens=200
    )

    output = response.generated_text if hasattr(response, "generated_text") else response[0]['generated_text']

    # You can parse the output better later
    return {
        "credibility_score": "AI-generated",
        "summary": output,
        "counterarguments": "Generated by AI"
    }

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

# 1️⃣ Define FastAPI app first
app = FastAPI()

# 2️⃣ Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://house-of-prompts.web.app"],  # frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 3️⃣ Load HF token from .env
load_dotenv()
HF_TOKEN = os.getenv("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")  # <- correct environment variable name
client = InferenceClient(token="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")

# 4️⃣ Define request model
class Article(BaseModel):
    article: str

# 5️⃣ Define route
@app.post("/analyze")
async def analyze(article: Article):
    text = article.article
    if not text.strip():
        return {"credibility_score": None, "summary": "Empty input.", "counterarguments": "N/A"}

    prompt = f"""
    Analyze the credibility of this news article and give:
    1. Credibility score out of 100
    2. Short summary
    3. Counterarguments against its claims

    Article: {text}
    """

    response = client.text_generation(
        model="gpt2",  # Replace with any suitable HF model
        inputs=prompt,
        max_new_tokens=200
    )

    output = response.generated_text if hasattr(response, "generated_text") else response[0]['generated_text']

    return {
        "credibility_score": "AI-generated",
        "summary": output,
        "counterarguments": "Generated by AI"
    }
